{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":527034,"sourceType":"datasetVersion","datasetId":250610}],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTING LIBRARIES AND PACKAGES**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport tensorflow as tf\nimport datetime\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:12:45.16117Z","iopub.execute_input":"2021-05-22T12:12:45.161534Z","iopub.status.idle":"2021-05-22T12:12:50.095077Z","shell.execute_reply.started":"2021-05-22T12:12:45.161459Z","shell.execute_reply":"2021-05-22T12:12:50.094163Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**LOADING DATASET**\n\n* using 256x256 images from MSCOCO dataset\n* X is array containing input images generated by resizing original images to 128x128 size\n* Y is array containing original images of 256x256 size\n* Only 1000 images are used out of 80K images for faster computation","metadata":{}},{"cell_type":"code","source":"X = []\nY = []\n\nc = 0;\ndata_size = 1000\ndirname = \"../input/mscoco/mscoco_resized/train2014\"\nfor filename in tqdm(os.listdir(dirname)):\n    im  = Image.open(os.path.join(dirname, filename))\n    Y.append(np.array(im))\n    im = im.resize((128,128))\n    X.append(np.array(im))\n    \n    c+=1\n    if(c == data_size+20):\n        break\n    \nX = np.array(X, dtype = 'float32')\nY = np.array(Y, dtype = 'float32')\n\nX = (X/127.5)-1\nY = (Y/127.5)-1\n\nX_train = X[0:data_size].reshape(-1,1,128,128,3)\nY_train = Y[0:data_size].reshape(-1,1,256,256,3)\n\nX_test = X[data_size:].reshape(-1,1,128,128,3)\nY_test = Y[data_size:].reshape(-1,1,256,256,3)\n\nprint(X_train.shape)\nprint(Y_train.shape)\n\nprint(X_test.shape)\nprint(Y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:12:50.096495Z","iopub.execute_input":"2021-05-22T12:12:50.096821Z","iopub.status.idle":"2021-05-22T12:12:59.747106Z","shell.execute_reply.started":"2021-05-22T12:12:50.096764Z","shell.execute_reply":"2021-05-22T12:12:59.746182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CREATING GENERATOR MODEL**\n\n* GAN uses two models, a generator model that generates output and a discriminator model that classifies wether the output is generated by the generator model(fake) or taken from the dataset(real)\n\n* The generator model is a U-Net. It is a neural network used for image to image tasks. It has three major components : downsampling blocks, upsampling blocks and skip connections.\n\n* Downsampling blocks convert image input to tensors of lower dimesions until it becomes a 1D tensor. Upsampling blocks convert output of downsampling blocks back to image output. Skip connections provide connections between downsampling and upsampling blocks at each level.\n\n* Generator and discriminator compete against each other.","metadata":{}},{"cell_type":"code","source":"#downsampling block\n#Structure : Conv2D -> BatchNorm -> LeakyReLU\n\ndef downsample(filters, size, apply_batchnorm = True):\n    initializer = tf.random_normal_initializer(0. , 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n        \n    result.add(tf.keras.layers.LeakyReLU())\n    \n    return result\n\n#upsampling block\n#Structure : Conv2DTranspose -> BatchNorm -> Dropout -> ReLU\n\ndef upsample(filters, size, apply_dropout = True):\n    initializer = tf.random_normal_initializer(0. , 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n    \n    result.add(tf.keras.layers.ReLU())\n    \n    return result\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:12:59.750428Z","iopub.execute_input":"2021-05-22T12:12:59.750709Z","iopub.status.idle":"2021-05-22T12:12:59.759095Z","shell.execute_reply.started":"2021-05-22T12:12:59.750682Z","shell.execute_reply":"2021-05-22T12:12:59.758114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Generator():\n    inputs = tf.keras.layers.Input(shape = [128,128,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm = False), #(bs,64,64,64)\n        downsample(128, 4), #(bs,32,32,128)\n        downsample(256, 4), #(bs,16,16,256)\n        downsample(512, 4), #(bs,8,8,512)\n        downsample(512, 4), #(bs,4,4,512)\n        downsample(512, 4), #(bs,2,2,512)\n        downsample(512, 4), #(bs,1,1,512)\n    ] \n    #each downsampling reduces size by 2 because of stride = 2,\n    #bs = batch size, 4th value is number of filters\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout = True), #(bs,2,2,1024)\n        upsample(512, 4, apply_dropout = True), #(bs,4,4,1024)\n        upsample(512, 4, apply_dropout = True), #(bs,8,8,1024)\n        upsample(512, 4), #(bs,16,16,1024)\n        upsample(256, 4), #(bs,32,32,512)\n        upsample(128, 4), #(bs,64,64,256)\n        \n    ]\n    \n    initializer = tf.random_normal_initializer(0. , 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3,4,strides=4,padding='same',kernel_initializer = initializer,activation = 'tanh')#(bs,256,256,3)                                      \n    \n    x = inputs\n    \n    \n    skips = []\n    \n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    \n    for up,skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x,skip])\n    \n    x = last(x)\n    \n    return tf.keras.Model(inputs = inputs, outputs = x)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:12:59.760837Z","iopub.execute_input":"2021-05-22T12:12:59.761272Z","iopub.status.idle":"2021-05-22T12:12:59.773711Z","shell.execute_reply.started":"2021-05-22T12:12:59.761223Z","shell.execute_reply":"2021-05-22T12:12:59.772607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:12:59.775417Z","iopub.execute_input":"2021-05-22T12:12:59.775882Z","iopub.status.idle":"2021-05-22T12:13:02.727189Z","shell.execute_reply.started":"2021-05-22T12:12:59.775843Z","shell.execute_reply":"2021-05-22T12:13:02.724557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GENERATOR LOSS**\n\n* Generator loss consists of two components\n* L1 loss which is mean absolute error between the generated image and target image to make generated images structurally similar to target images\n* GAN loss which is binary crossentropy loss of discriminator's output on generated images and array of ones.\n* Total loss = GAN loss + (LAMBDA * L1 loss)","metadata":{}},{"cell_type":"code","source":"LAMBDA = 500\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n    \n    l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n    \n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n    \n    return total_gen_loss","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:02.728536Z","iopub.execute_input":"2021-05-22T12:13:02.729012Z","iopub.status.idle":"2021-05-22T12:13:02.735904Z","shell.execute_reply.started":"2021-05-22T12:13:02.728969Z","shell.execute_reply":"2021-05-22T12:13:02.735076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CREATING DISCRIMINATOR MODEL**\n\n* Discriminator model is a PatchGAN\n* In a PatchGAN, the output is a 3D vector referring to similarity between patches of input and target images\n* Model consists of downsampling blocks : Conv->BatchNorm->LeakyReLU\n* It receives two inputs : Input image and generated image which is classified as fake and input image and target image which is classified as real\n","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[128,128,3], name='input_image')\n    tar = tf.keras.layers.Input(shape=[256,256,3], name='target_image')\n    \n    inp_resized = tf.image.resize(inp, (256,256), method = 'bicubic')\n    \n    x = tf.keras.layers.concatenate([inp_resized,tar]) #(bs,256,256,6)\n    x = downsample(64,4,False)(x) #(bs,128,128,64)\n    x = downsample(128,4)(x) #(bs,64,64,128)\n    x = downsample(256,4)(x) #(bs,32,32,256)\n    \n    x = tf.keras.layers.ZeroPadding2D()(x) #(bs,34,34,256)\n    x = tf.keras.layers.Conv2D(512,4,strides=1,kernel_initializer=initializer,use_bias=False)(x) #(bs,31,31,512)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n    x = tf.keras.layers.ZeroPadding2D()(x) #(bs,33,33,512)\n    x = tf.keras.layers.Conv2D(1,4,strides=1,kernel_initializer=initializer)(x) #(bs,30,30,1)\n    \n    return tf.keras.Model(inputs = [inp,tar], outputs = x)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:02.737286Z","iopub.execute_input":"2021-05-22T12:13:02.737937Z","iopub.status.idle":"2021-05-22T12:13:02.74926Z","shell.execute_reply.started":"2021-05-22T12:13:02.737899Z","shell.execute_reply":"2021-05-22T12:13:02.748477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:02.752037Z","iopub.execute_input":"2021-05-22T12:13:02.752424Z","iopub.status.idle":"2021-05-22T12:13:03.072282Z","shell.execute_reply.started":"2021-05-22T12:13:02.752389Z","shell.execute_reply":"2021-05-22T12:13:03.071375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DISCRIMINATOR LOSS**\n\n* It takes two inputs : discriminator output for real images and generated images and it has two components : real loss and generated loss\n* real loss is sigmoid cross entropy loss of real image output and array of ones\n* generated loss is sigmoid cross entropy loss of generated image output and array of zeros\n* total loss is sum of real loss and generated loss","metadata":{}},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_gen_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n    \n    gen_loss = loss_object(tf.zeros_like(disc_gen_output), disc_gen_output)\n    \n    return real_loss + gen_loss","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:03.074023Z","iopub.execute_input":"2021-05-22T12:13:03.074285Z","iopub.status.idle":"2021-05-22T12:13:03.082037Z","shell.execute_reply.started":"2021-05-22T12:13:03.074258Z","shell.execute_reply":"2021-05-22T12:13:03.081264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**OPTIMIZERS AND CHECKPOINT SAVER**","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n\ncheckpoint_dir = \"./training_checkpoints\"\ncheckpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n                                discriminator_optimizer = discriminator_optimizer,\n                                generator = generator,\n                                discriminator = discriminator)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:03.084212Z","iopub.execute_input":"2021-05-22T12:13:03.084487Z","iopub.status.idle":"2021-05-22T12:13:03.093052Z","shell.execute_reply.started":"2021-05-22T12:13:03.084463Z","shell.execute_reply":"2021-05-22T12:13:03.092157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FUNCTION TO GENERATE IMAGES**","metadata":{}},{"cell_type":"code","source":"def generate_images(model, inp, tar):\n    inp_normalized = (inp/127.5)-1\n    pred = model(inp_normalized, training = True)\n    pred = (pred+1)*127.5\n    \n    pred = Image.fromarray(tar[0].astype('uint8'),'RGB')\n    pred = pred.resize((200,200))\n    pred = np.array(pred).reshape((1,200,200,3))\n    display_list = [np.array(inp[0], dtype='int'),np.array(pred[0], dtype='int'),np.array(tar[0], dtype='int')]\n    title_list = ['input','prediction','target']\n    plt.figure(figsize = (20,20))\n    \n    for i in range(3):\n        plt.subplot(1,3,i+1)\n        plt.title(title_list[i])\n        plt.imshow(display_list[i])\n        plt.axis('off')\n    \n    plt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:03.094514Z","iopub.execute_input":"2021-05-22T12:13:03.094913Z","iopub.status.idle":"2021-05-22T12:13:03.105704Z","shell.execute_reply.started":"2021-05-22T12:13:03.094878Z","shell.execute_reply":"2021-05-22T12:13:03.10485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TRAINING**","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\n@tf.function\ndef train_step(inp, tar, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(inp, training = True)\n        \n        disc_real_output = discriminator([inp,tar], training = True)\n        disc_gen_output = discriminator([inp,gen_output], training = True)\n        \n        gen_loss = generator_loss(disc_gen_output, gen_output, tar)\n        disc_loss = discriminator_loss(disc_real_output, disc_gen_output)\n    \n    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n\ndef fit(X_train, Y_train, X_test, Y_test, epochs):\n    for epoch in range(epochs):\n        clear_output(wait = True)\n        \n        #generate_images(generator, (X_test[0]+1)*127.5, (Y_test[0]+1)*127.5)\n\n        print(\"Epoch : \", epoch)\n        \n        for inp,tar in tqdm(zip(X_train,Y_train)):\n            train_step(inp,tar,epoch)\n        \n        if (epoch+1)%10 == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n        \n    checkpoint.save(file_prefix=checkpoint_prefix)\n\nfit(X_train, Y_train, X_test, Y_test, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:13:03.107085Z","iopub.execute_input":"2021-05-22T12:13:03.107624Z","iopub.status.idle":"2021-05-22T12:19:50.780275Z","shell.execute_reply.started":"2021-05-22T12:13:03.107586Z","shell.execute_reply":"2021-05-22T12:19:50.779097Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_images(generator, (X_test[1]+1)*127.5, (Y_test[1]+1)*127.5)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T12:19:50.786799Z","iopub.execute_input":"2021-05-22T12:19:50.790666Z","iopub.status.idle":"2021-05-22T12:19:51.254633Z","shell.execute_reply.started":"2021-05-22T12:19:50.790616Z","shell.execute_reply":"2021-05-22T12:19:51.253678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}